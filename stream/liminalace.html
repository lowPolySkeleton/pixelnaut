<!DOCTYPE html>
<html>

<head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>liminalace</title>
	<style>
		*{
		box-sizing: border-box;
		}
		html, body{
			margin: 0;
			padding: 0;
			height: 100%;
			overflow: hidden;
		}
		#canvas{
			background-image: url("images/CG-static.gif");
			background-position: center bottom;
			background-size: 1600px;
			background-repeat: no-repeat;
			transition: .3s;
			width: 100%;
			height: 100%;
			transform-origin: center bottom;
			image-rendering:optimizeQuality;
		}
		#shake{
			width: 100%;
			height: 100%;
			transform-origin: bottom center;
		}
		.move{
			background-image: url("images/CG-talk.gif") !important;
		}
		.shake{
			animation: shake 0.5s infinite;
		}
		@keyframes shake {
			0% { transform: translate(3px, 3px) rotate(0deg); }
			10% { transform: translate(-3px, -4px) rotate(-3deg); }
			20% { transform: translate(-5px, 0px) rotate(3deg); }
			30% { transform: translate(5px, 4px) rotate(0deg); }
			40% { transform: translate(3px, -3px) rotate(3deg); }
			50% { transform: translate(-3px, 4px) rotate(-3deg); }
			60% { transform: translate(-5px, 3px) rotate(0deg); }
			70% { transform: translate(5px, 3px) rotate(-3deg); }
			80% { transform: translate(-3px, -3px) rotate(3deg); }
			90% { transform: translate(3px, 5px) rotate(0deg); }
			100% { transform: translate(3px, -4px) rotate(-3deg); }
		}
	</style>
	
</head>

<body>
	<div id="shake">
	<div id="canvas"></div>
</div>


	


	
	<script>
		navigator.getUserMedia = navigator.getUserMedia ||
		navigator.webkitGetUserMedia ||
		navigator.mozGetUserMedia;
		if (navigator.getUserMedia) {
		navigator.getUserMedia({
			audio: true
			},
			function(stream) {
			audioContext = new AudioContext();
			analyser = audioContext.createAnalyser();
			microphone = audioContext.createMediaStreamSource(stream);
			javascriptNode = audioContext.createScriptProcessor(2048, 1, 1);

			analyser.smoothingTimeConstant = 0.8;
			analyser.fftSize = 1024;

			microphone.connect(analyser);
			analyser.connect(javascriptNode);
			javascriptNode.connect(audioContext.destination);

			canvasContext = document.getElementById("canvas")
			shakeContext = document.getElementById("shake")

			javascriptNode.onaudioprocess = function() {

				
				var array = new Uint8Array(analyser.frequencyBinCount);
				analyser.getByteFrequencyData(array);
				var values = 0;

				var length = array.length;
				for (var i = 0; i < length; i++) {
					values += (array[i]);
				}
				
				if (values >= 7000){
					var scale = values / 100000
					console.log("scale("+ (Number(scale) + 1) +")")
					canvasContext.classList.add("move");
					if((Number(scale) + 1) > 1.3){
						shakeContext.classList.add("shake");
					}else{
						shakeContext.classList.remove("shake");
					}
					canvasContext.style.transform = "scale("+ (Number(scale) + 1) +")"
				}else{
					canvasContext.classList.remove("move");
					
					canvasContext.style.transform = "scale(1)"
				}


				} // end fn stream
			},
			function(err) {
			console.log("The following error occured: " + err.name)
			});
		} else {
		console.log("getUserMedia not supported");
		}

		function runSpeechRecognition() {
                // new speech recognition object
                var SpeechRecognition = SpeechRecognition;
                var recognition = new SpeechRecognition();
				canvasContext = document.getElementById("canvas")
            
                
                recognition.onspeechend = function() {
                    recognition.stop();
					runSpeechRecognition()
                }
              
               
              
                 // start recognition
                 recognition.start();
	        }


			// runSpeechRecognition()
	</script>
</body>
</html>
